# LLM-IR-Bias-Fairness

Feel free to contact us if you find a mistake or have any advice.

## ðŸ“‹ Table of Content

## ðŸ“„ Paper List

### Bias
#### Bias in Data Collection
1. **LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.20501) ![](https://img.shields.io/badge/Source_Bias-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
2. **AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2311.14084) ![](https://img.shields.io/badge/Source_Bias-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
3. 

#### Bias in Model Development
1. **Large Language Models are Zero-Shot Rankers for Recommender Systems**, ECIR 2024. [[Paper]](https://arxiv.org/abs/2305.08845) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan) ![](https://img.shields.io/badge/Prompting-darkcyan)
2. **Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.07712) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
3. **RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2312.16018) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
4. **Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2306.17563) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
5. **Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2307.05722) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
6. **Large Language Models are Not Stable Recommender Systems**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2312.15746) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
7. **A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2308.08434) ![](https://img.shields.io/badge/Popularity_Bias-orange)
8. **Large Language Models as Zero-Shot Conversational Recommenders**, CIKM 2023. [[Paper]](https://arxiv.org/abs/2308.10053) ![](https://img.shields.io/badge/Popularity_Bias-orange)
9. **Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation**, EMNLP 2023. [[Paper]](https://arxiv.org/abs/2310.16738) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
10. **Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2401.10545) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)
11. **ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2401.03605) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)


#### Bias in Result Evaluation
1. **Large Language Models Are Not Robust Multiple Choice Selectors**, ICLR 2024. [[Paper]](https://arxiv.org/abs/2309.03882) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
2. **Humans or LLMs as the Judge? A Study on Judgement Biases**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2402.10669) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Style_Bias-orange)
3. **Benchmarking Cognitive Biases in Large Language Models as Evaluators**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2309.17012) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Egocentric_Bias-orange)
4. **Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2308.11483) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
5. **Large Language Models are not Fair Evaluators**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2305.17926) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Rebalancing-darkcyan) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
6. **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, NeurIPS 2023. [[Paper]](https://arxiv.org/abs/2306.05685) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
7. **Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2401.16788) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
8. **EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria**, CHI 2024. [[Paper]](https://arxiv.org/abs/2309.13633) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)
9. **LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2311.09766) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Egocentric_Bias-orange)
10. **Verbosity Bias in Preference Labeling by Large Language Models**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.10076) ![](https://img.shields.io/badge/Style_Bias-orange)
11. **Style Over Substance: Evaluation Biases for Large Language Models**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.10076) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)
12. **An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2403.02839) ![](https://img.shields.io/badge/Style_Bias-orange)
13. **G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2303.16634) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
14. **PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2307.02762) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
15. **ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2309.13701) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)









### Unfairness
#### Unfairness in Data Collection
1. **Measuring and Mitigating Unintended Bias in Text Classification**, AIES '18. [[Paper]](https://dl.acm.org/doi/10.1145/3278721.3278729) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
2. **Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models**, ACL '23. [[Paper]](https://aclanthology.org/2023.findings-acl.336/) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
3. **Gender Bias in Neural Natural Language Processing**, Preprint 2019. [[Paper]](https://arxiv.org/abs/1807.11714) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
4. **MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions**, ACL '23. [[Paper]](https://aclanthology.org/2023.acl-long.123/) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
5. **SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures**, ACL '22. [[Paper]](https://aclanthology.org/2022.acl-long.447/) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
6. **Do LLMs Implicitly Exhibit User Discrimination in Recommendation? An Empirical Study**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2311.07054) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
7. **Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation**, Recsys '23. [[Paper]](https://dl.acm.org/doi/10.1145/3604915.3608860) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
8. **Mitigating harm in language models with conditional-likelihood filtration**, Preprint 2021. [[Paper]](https://arxiv.org/abs/2108.07790) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Filtering-darkcyan)
9. **Exploring the limits of transfer learning with a unified text-to-text transformer**, JMLR. [[Paper]](https://jmlr.org/papers/volume21/20-074/20-074.pdf) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Data_Filtering-darkcyan)
10. **CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2403.05668) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
11. **BLIND: Bias Removal With No Demographics**, ACL '23. [[Paper]](https://aclanthology.org/2023.acl-long.490/) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
12. **Identifying and Reducing Gender Bias in Word-Level Language Models**, NAACL '19. [[Paper]]([https://aclanthology.org/2023.acl-long.490/])(https://aclanthology.org/P19-2031.pdf)) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
13. **Reducing Sentiment Bias in Language Models via Counterfactual Evaluation**ï¼Œ Findings-EMNLP' 20. [[Paper]]([https://aclanthology.org/2023.acl-long.490/])(https://aclanthology.org/2020.findings-emnlp.7/)) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
14. **Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function**, ACL-workshop '19. [[Paper]]([https://aclanthology.org/P19-2031/])(https://aclanthology.org/2020.findings-emnlp.7/)) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
15. **Bias of AI-Generated Content: An Examination of News Produced by Large Language Models**, Preprint 2023. [[Paper]]([https://arxiv.org/abs/2309.09825])(https://aclanthology.org/2020.findings-emnlp.7/)) ![](https://img.shields.io/badge/User_Unfairness-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)

#### Unfairness in Model Development
1. 

#### Unfairness in Result Evaluation
