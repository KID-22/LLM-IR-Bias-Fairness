# LLM-IR-Bias-Fairness

Feel free to contact us if you find a mistake or have any advice.

## ðŸ“‹ Table of Content

## ðŸ“„ Paper List

### Bias
#### Bias in Data Collection
1. **LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.20501) ![](https://img.shields.io/badge/Source_Bias-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
2. **AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2311.14084) ![](https://img.shields.io/badge/Source_Bias-orange) ![](https://img.shields.io/badge/Regularization-darkcyan)
3. 

#### Bias in Model Development
1. **Large Language Models are Zero-Shot Rankers for Recommender Systems**, ECIR 2024. [[Paper]](https://arxiv.org/abs/2305.08845) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan) ![](https://img.shields.io/badge/Prompting-darkcyan)
2. **Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.07712) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
3. **RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2312.16018) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
4. **Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2306.17563) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
5. **Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2307.05722) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)![](https://img.shields.io/badge/Rebalancing-darkcyan)
6. **Large Language Models are Not Stable Recommender Systems**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2312.15746) ![](https://img.shields.io/badge/Position_Bias-orange) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
7. **A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2308.08434) ![](https://img.shields.io/badge/Popularity_Bias-orange)
8. **Large Language Models as Zero-Shot Conversational Recommenders**, CIKM 2023. [[Paper]](https://arxiv.org/abs/2308.10053) ![](https://img.shields.io/badge/Popularity_Bias-orange)
9. **Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation**, EMNLP 2023. [[Paper]](https://arxiv.org/abs/2310.16738) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
10. **Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2401.10545) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)
11. **ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2401.03605) ![](https://img.shields.io/badge/Popularity_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)


#### Bias in Result Evaluation
1. **Large Language Models Are Not Robust Multiple Choice Selectors**, ICLR 2024. [[Paper]](https://arxiv.org/abs/2309.03882) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
2. **Humans or LLMs as the Judge? A Study on Judgement Biases**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2402.10669) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Style_Bias-orange)
3. **Benchmarking Cognitive Biases in Large Language Models as Evaluators**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2309.17012) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Egocentric_Bias-orange)
4. **Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2308.11483) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
5. **Large Language Models are not Fair Evaluators**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2305.17926) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Rebalancing-darkcyan) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
6. **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, NeurIPS 2023. [[Paper]](https://arxiv.org/abs/2306.05685) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
7. **Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2401.16788) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
8. **EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria**, CHI 2024. [[Paper]](https://arxiv.org/abs/2309.13633) ![](https://img.shields.io/badge/Selection_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)
9. **LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2311.09766) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Egocentric_Bias-orange)
10. **Verbosity Bias in Preference Labeling by Large Language Models**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.10076) ![](https://img.shields.io/badge/Style_Bias-orange)
11. **Style Over Substance: Evaluation Biases for Large Language Models**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2310.10076) ![](https://img.shields.io/badge/Style_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)
12. **An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers**, Preprint 2024. [[Paper]](https://arxiv.org/abs/2403.02839) ![](https://img.shields.io/badge/Style_Bias-orange)
13. **G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2303.16634) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan) ![](https://img.shields.io/badge/Rebalancing-darkcyan)
14. **PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2307.02762) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Data_Augmentation-darkcyan)
15. **ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning**, Preprint 2023. [[Paper]](https://arxiv.org/abs/2309.13701) ![](https://img.shields.io/badge/Egocentric_Bias-orange) ![](https://img.shields.io/badge/Prompting-darkcyan)









### Unfairness
#### Unfairness in Data Collection
1. 

#### Unfairness in Model Development
1. 

#### Unfairness in Result Evaluation
